{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio embeddings using deep LSTM autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the UrbanSound8K metadata from which we will get our training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from helpers import wav_to_floats\n",
    "\n",
    "audio_path = \"..\\\\Data\\\\UrbanSound8K\\\\audio\"\n",
    "metadata_path = \"..\\\\Data\\\\UrbanSound8K\\\\metadata\\\\UrbanSound8K.csv\"\n",
    "\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# For prototyping, we will select a small sample of sounds\n",
    "# metadata = metadata.sample(3000)\n",
    "\n",
    "# Display the data format\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a method which will grab an UrbanSound8K slice file and return it as a float array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_audio_from_dataframe(index, fold, slice_file_name):\n",
    "    clip_path = os.path.join(audio_path,\"fold\" + str(fold), slice_file_name)\n",
    "    audio = np.array(wav_to_floats(clip_path))\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform dilated gammatone analysis on each clip, store the values in a Pandas DataFrame and save the values as CSV files. We also make a DataFrame for holding reference to each CSV file name and their associated UrbanSound class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set params\n",
    "sample_rate = 44100\n",
    "window_size_seconds = 0.01\n",
    "num_filters = 100\n",
    "cutoff_low_hz = 30\n",
    "frames_in_memory = 8\n",
    "\n",
    "# Prepare dilated gammatone processing\n",
    "from gammatone_filterbank import GammatoneFilterbank\n",
    "filterbank = GammatoneFilterbank(sample_rate, window_size_seconds, window_size_seconds / 2, num_filters, cutoff_low_hz)\n",
    "\n",
    "# Data paths\n",
    "save_path = \"..\\\\Data\\\\SpectralFrames\"\n",
    "references_path = os.path.join(save_path, \"references.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Make dataframe for storing references\n",
    "refs_list = []\n",
    "sound_num = 1\n",
    "\n",
    "for index, row in metadata.iterrows():\n",
    "    print(\"Processing sound \" + str(sound_num) + \" of 8000\")\n",
    "    sound_num += 1\n",
    "    \n",
    "    if sound_num > 8100:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        path = os.path.join(save_path, row[\"slice_file_name\"].replace(\"wav\", \"csv\"))\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            audio = get_audio_from_dataframe(index, row[\"fold\"],  row[\"slice_file_name\"])\n",
    "            frames = filterbank.make_dilated_spectral_frames(audio, frames_in_memory, 2)\n",
    "            frames = frames/frames.max()\n",
    "\n",
    "            # Flatten the 3D array made by dilated gammatone filter and use multi-index for dataframe\n",
    "            multi_index = pd.MultiIndex.from_product([range(s) for s in frames.shape])\n",
    "            frames = pd.DataFrame(frames.flatten(), index=multi_index).reset_index()\n",
    "\n",
    "            frames.to_csv(path)\n",
    "        \n",
    "        ref_dict = {\"original_file_name\": row[\"slice_file_name\"], \"new_file_name\": row[\"slice_file_name\"].replace(\"wav\", \"csv\"), \"class_id\": row[\"classID\"], \"class_name\": row[\"class\"]}\n",
    "        refs_list.append(ref_dict)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error reading audio file, moving to next\")\n",
    "                                                                  \n",
    "references = pd.DataFrame(refs_list)\n",
    "references.to_csv(references_path)\n",
    "\n",
    "references.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for getting spectral data back from CSV, restoring its dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spectral_frames_from_csv(path):\n",
    "    data = pd.read_csv(path, index_col=[0,1,2,3], dtype=np.float32)\n",
    "    dim1 = len(data.index.get_level_values(1).unique())\n",
    "    dim2 = len(data.index.get_level_values(2).unique())\n",
    "    dim3 = len(data.index.get_level_values(3).unique())\n",
    "    reshaped_data = data.values.reshape((dim1, dim2, dim3))\n",
    "\n",
    "    return reshaped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some examples from the training data to vizualize the dilated spectral buffer. The horizontal axis represents frequency content and the vertical axis represents time with the bottom being the most recent timestep and the top being the least recent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example_train_path = os.path.join(save_path, \"61626-9-0-6.csv\")\n",
    "example_train = get_spectral_frames_from_csv(example_train_path)\n",
    "example_val_path = os.path.join(save_path, \"189989-0-0-0.csv\")\n",
    "example_val = get_spectral_frames_from_csv(example_val_path)\n",
    "\n",
    "# Plot and print data\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xscale('symlog')\n",
    "plt.pcolormesh(example_train[100])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xscale('symlog')\n",
    "plt.pcolormesh(example_val[100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to try and use these frames of dilated frequencies to create usable temporal audio embeddings which can be used in neural networks. Ideally, the temporal context of the frequencies at a current moment, should be compressed down into a smaller unidimensional space. We prepare an LSTM Autoencoder for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from model import prepare_autoencoder\n",
    "from keras import optimizers\n",
    "autoencoder, encoder = prepare_autoencoder(frames_in_memory, \n",
    "                                           num_filters, \n",
    "                                           250, \n",
    "                                           \"adagrad\", \n",
    "                                           \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spectral_generator(reference_csv_path, spectral_csv_directory, batch_size):\n",
    "    refs = pd.read_csv(reference_csv_path, index_col=0)\n",
    "    ref_index = 0\n",
    "    \n",
    "    spectral_path = os.path.join(spectral_csv_directory, refs.iloc[ref_index][\"new_file_name\"])\n",
    "    spectral_frames = get_spectral_frames_from_csv(spectral_path)\n",
    "    frame_index = 0\n",
    "    num_frames = spectral_frames.shape[1]\n",
    "    num_filters = spectral_frames.shape[2]\n",
    "    \n",
    "    batch_features = np.zeros((batch_size, num_frames, num_filters))\n",
    "    \n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            if (frame_index >= spectral_frames.shape[0]):\n",
    "                \n",
    "                frame_index = 0\n",
    "                ref_index += 1\n",
    "                \n",
    "                if (ref_index >= 8000):\n",
    "                    ref_index = 0\n",
    "                \n",
    "                spectral_path = os.path.join(spectral_csv_directory, refs.iloc[ref_index][\"new_file_name\"])\n",
    "                spectral_frames = get_spectral_frames_from_csv(spectral_path)\n",
    "            \n",
    "            # noise = (np.random.random_sample(spectral_frames[frame_index].shape) * 0.01)\n",
    "            batch_features[i] = spectral_frames[frame_index] # * noise\n",
    "            frame_index += 1        \n",
    "        \n",
    "        yield batch_features, batch_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the autoencoder model has been prepared, let's fit it and plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights(\"C:/Users/Rothmann/Documents/PROJECTS/Development/Thesis/Keras/Models/LSTM_Autoencoder/autoencoder_weights.h5\")\n",
    "\n",
    "gen_batch_size = 1000\n",
    "steps_to_take = 8000 * 700 / gen_batch_size # num sounds * average num frames per sounds / batch size\n",
    "spectral_gen = spectral_generator(references_path, save_path, gen_batch_size)\n",
    "\n",
    "history = autoencoder.fit_generator(spectral_gen, \n",
    "                                    steps_per_epoch=steps_to_take, \n",
    "                                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "#  \"Accuracy\"\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "# \"Loss\"\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been fit, let's save its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_path = get_save_path()\n",
    "autoencoder.save_weights(\"C:/Users/Rothmann/Documents/PROJECTS/Development/Thesis/Keras/Models/LSTM_Autoencoder/autoencoder_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict some encodings to compare the encoded data with the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = autoencoder.predict(x=example_train, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll plot a few of these encodings to get an idea of how well it worked. Looks like the temporal context is reconstructing quite well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from random import *\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "\n",
    "for i in range(3):\n",
    "    rand_example = randint(1, 100)\n",
    "    \n",
    "    plt.subplot(3, 2, i * 2 + 1)\n",
    "    plt.xscale('symlog')\n",
    "    if i == 0:\n",
    "        plt.title('original')\n",
    "    plt.pcolormesh(example_train[rand_example])\n",
    "    \n",
    "    plt.subplot(3, 2,  i * 2 + 2)\n",
    "    plt.xscale('symlog')\n",
    "    if i == 0:\n",
    "        plt.title('predicted')\n",
    "    plt.pcolormesh(prediction[rand_example])\n",
    "     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
